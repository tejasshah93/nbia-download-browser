-------------------------
title: Week 3  
date: 2015-06-17 14:30:00  
NOTES  
-------------------------

### Week 3 Report:
  - Tar stream downloaded using [node http.request](https://nodejs.org/api/http.html#http_http_request_options_callback).
  Since all the npm modules used are transformed into browser compatible methods
  using [Browserify](http://browserify.org/), this http.request is done using
  `XMLHttpRequest (XHR)` at the backend when run on the browser.
  - The main crux now was to parse the tar stream as and when it comes i.e.
  on-the-fly approach. The [node-tar](https://github.com/npm/node-tar) mentioned
  in previous post wasn't browserifyable, so its alternative [tar-stream](https://github.com/mafintosh/tar-stream)
  npm module is used instead.
  - Initially something like `res.pipe(tar.extract())` was used so as to pipe
  the http response object directly into tar parser but it resulted into this
  error: [http://stackoverflow.com/questions/30869460/node-js-browserify-error-on-parsing-tar-file]
  Summarizing the problem, the code when executed on terminal using node.js
  fetches and parses the tar successfully. But when the same code is
  browserified, it resulted in parsing 2 out of 3 files and raised `Error:
  Invalid tar header. Maybe the tar is corrupted or it needs to be gunzipped?`
  for the 3rd file. Now this was chaos!
  - The above issue resulted in long quest of probably 2 days for encoding
  problems, but finally was resolved after some deep discussions on #node.js IRC
  (problem cause mentioned in the answer to the Stack Overflow question in
  detail). Basically, the `xhr responseType` was to be set as `arraybuffer` but
  the browserify stream API takes `Buffer` as input. So, we've to transform the
  chunk into `new Buffer(chunk)` on receiving them.  
  FYI: A couple issues were created at the github respective repos too!
    - [https://github.com/substack/stream-browserify/issues/8]
    - [https://github.com/substack/http-browserify/issues/89]
  - Result: Now, the tar stream gets downloaded and parsed successfully (all of
  the files within) on-the-fly using npm modules and browserify. Yay! :)
  - Demonstration: I've hosted the implementation [here](http://researchweb.iiit.ac.in/~tejas.shah/gsoc15/browserBased/).
  For now, I've kept the tarstream downloaded on the hosting domain itself and
  its not currently fetched from cancerimagingarchive.net because it was leading
  to an unresolvable `Access-Control-Allow-Origin` error for Cross Domains.
  Tried a few workarounds for CORS but didn't work at time.
  - Also made myself familiar with the Minimongo. Pretty straight-forward (it
  being a JavaScript API for MongoDB).

### Roadmap Ahead:
  - Integration of the above implemented with Minimongo
    + Maintaining the series UID, SOP UIDs for the particular tar stream and
    tracking progress of what's downloaded. Persistent storage.
    + Using IndexedDB backed Minimongo interface.
  - Exporting the files to client's FS:
    + A similar question [here](http://stackoverflow.com/questions/19802032/how-can-a-chrome-extension-save-many-files-to-a-user-specified-directory)
    but that's related to Chrome Extension.
    + Possible alternatives to be discussed.  

  - **Post -discussion**: Mail stating the current known possible solutions for
  exporting files to client's File System which are browser specific. (Scaling
  will be done, later)

### [Post-Discussion] TO DOs
  - As mentioned in Roadmap, integrate the code with Minimongo IndexedDB backed
  interface
  - Look out for exporting files and try to implement the same so as to download
  files after their entry is recorded in the database.
  - Cross Domain requests are still a pain. Work around needed. Manipulate some
  req/res headers till then and try to arrive at a conclusion.

================================================================================
